{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log _loss function AND sigmoid \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_log(x,y):\n",
    "    epsilon = 1e-10\n",
    "    y_p = [max(i,epsilon) for i in y]\n",
    "    y_p = [min(i,1-epsilon) for i in y_p]\n",
    "    print(y_p)\n",
    "    y_p = np.array(y_p)\n",
    "    return -np.mean(x*np.log(y_p)+(1-x)*np.log(1-y_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.13533528, 2.        , 1.36787944])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid_nm(X):\n",
    "    return 1/1+np.exp(-X)\n",
    "sigmoid_nm(np.array([2,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,1,0,1,1]).astype(int)\n",
    "y = np.array([0.30,0.7,1,0,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, 0.7, 0.9999999999, 1e-10, 0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.661099341193033"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_log(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent SGD AND MINI BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_NN:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1\n",
    "        self.w2 = 1\n",
    "        self.bais = 0\n",
    "    def fit(self,x,y,epochs,loss_thresshold):\n",
    "        self.w1 ,self,w2,self,bais = self.gradient_des(x['age'],x['afforadability'],y,epochs,loss_thresshold)\n",
    "    def predict(self,x_t):\n",
    "        weighted_sum = self.w1*x_t['age']+self.w2*x_t['afforadability']\n",
    "    def gradient_descent(self, age,affordability, y_true, epochs, loss_thresold):\n",
    "        w1 = w2 = 1\n",
    "        bias = 0\n",
    "        rate = 0.5\n",
    "        n = len(age)\n",
    "        for i in range(epochs):\n",
    "            weighted_sum = w1 * age + w2 * affordability + bias\n",
    "            y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            loss = log_loss(y_true, y_predicted)\n",
    "            \n",
    "            w1d = (1/n)*np.dot(np.transpose(age),(y_predicted-y_true)) \n",
    "            w2d = (1/n)*np.dot(np.transpose(affordability),(y_predicted-y_true)) \n",
    "\n",
    "            bias_d = np.mean(y_predicted-y_true)\n",
    "            w1 = w1 - rate * w1d\n",
    "            w2 = w2 - rate * w2d\n",
    "            bias = bias - rate * bias_d\n",
    "            \n",
    "            if i%50==0:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "            \n",
    "            if loss<=loss_thresold:\n",
    "                print (f'Epoch:{i}, w1:{w1}, w2:{w2}, bias:{bias}, loss:{loss}')\n",
    "                break\n",
    "\n",
    "        return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n",
    " \n",
    "    number_of_features = X.shape[1]\n",
    "    # numpy array with 1 row and columns equal to number of features. In \n",
    "    # our case number_of_features = 3 (area, bedroom and age)\n",
    "    w = np.ones(shape=(number_of_features)) \n",
    "    b = 0\n",
    "    total_samples = X.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    for i in range(epochs):    \n",
    "        random_index = random.randint(0,total_samples-1) # random index from total samples\n",
    "        sample_x = X[random_index]\n",
    "        sample_y = y_true[random_index]\n",
    "        \n",
    "        y_predicted = np.dot(w, sample_x.T) + b\n",
    "    \n",
    "        w_grad = -(2/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n",
    "        b_grad = -(2/total_samples)*(sample_y-y_predicted)\n",
    "        \n",
    "        w = w - learning_rate * w_grad\n",
    "        b = b - learning_rate * b_grad\n",
    "        \n",
    "        cost = np.square(sample_y-y_predicted)\n",
    "        \n",
    "        if i%100==0: # at every 100th iteration record the cost and epoch value\n",
    "            cost_list.append(cost)\n",
    "            epoch_list.append(i)\n",
    "        \n",
    "    return w, b, cost, cost_list, epoch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch(X, y_true, epochs, learning_rate = 0.01):\n",
    " \n",
    "    number_of_features = X.shape[1]\n",
    "    # numpy array with 1 row and columns equal to number of features. In \n",
    "    # our case number_of_features = 3 (area, bedroom and age)\n",
    "    w = np.ones(shape=(number_of_features)) \n",
    "    b = 0\n",
    "    total_samples = X.shape[0]\n",
    "    if batch_size >total_samples:\n",
    "        \n",
    "        cost_list = []\n",
    "        epoch_list = []\n",
    "        num_batches = total_samples/batch_size\n",
    "        for i in range(epochs):    \n",
    "            random_indices  = np.random.permutation(total_samples)\n",
    "            X_tmp = X[random_indices]\n",
    "            y_tmp = y_true[random_indices]\n",
    "            for j in range(0,total_samples,batch_size):\n",
    "                Xj = X_tmp[j:j+batch_size]\n",
    "                yj = y_tmp[j:j+batch_size]\n",
    "                y_predicted = np.dot(w, Xj.T) + b\n",
    "                w_grad = -(2/len(Xj))*(Xj.T.dot(yj-y_predicted))\n",
    "                b_grad = -(2/len(Xj))*np.sum(yj-y_predicted)\n",
    "\n",
    "            w = w - learning_rate * w_grad\n",
    "            b = b - learning_rate * b_grad\n",
    "\n",
    "            cost = np.square(yj-y_predicted)\n",
    "\n",
    "            if i%100==0: # at every 100th iteration record the cost and epoch value\n",
    "                cost_list.append(cost)\n",
    "                epoch_list.append(i)\n",
    "\n",
    "        return w, b, cost, cost_list, epoch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
